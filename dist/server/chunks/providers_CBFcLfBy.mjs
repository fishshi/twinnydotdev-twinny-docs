const id = "general/providers.md";
						const collection = "docs";
						const slug = "general/providers";
						const body = "\nThese example configurations serve as a starting point. Individual adjustments may be required depending on your specific hardware and software environments.\n\n\n> Note: twinny chat (not auto-complete) should be compatible with any API which adheres to the OpenAI API specification.\n\n\n### Ollama (Configured by default)\n\n#### Auto-complete\n\n- **Hostname:** `localhost`\n- **Port:** `11434`\n- **Path:** `/api/generate`\n- **Model Name:** `codellama:7b-code`\n- **FIM Template:** `codellama`\n\n#### Chat\n\n- **Hostname:** `localhost`\n- **Port:** `11434`\n- **Path:** `/v1/chat/completions`\n- **Model Name:** `codellama:7b-instruct` \n\n### Open WebUI using Ollama\n\nOpen WebUI can be used a proxy Ollama, simply configure the endpoint to match what is served by OpenWeb UI.\n\n#### Auto-complete\n\n- **Hostname:** `localhost`\n- **Port:** The port OpenWebUI is serving, typically `8080` or `3000`.\n- **Path:** `/ollama/api/generate`\n- **Model Name:** `codellama:7b-code`\n- **FIM Template:** Select a template that matches the model, such as `codellama` for `codellama:7b-code` or `deepseek` for `deepseek-coder`.\n\n#### Chat\n\n- **Hostname:** `localhost`\n- **Port:** The port OpenWebUI is serving, typically `8080` or `3000`.\n- **Path:** `/ollama/v1/chat/completions`\n- **Model Name:** `codellama:7b-instruct` or any effective instruct model.\n\n### LM Studio\n\n#### Auto-complete\n\n- **Hostname:** `localhost`\n- **Port:** `1234`\n- **Path:** `/v1/completions`\n- **Model Name:** Base model such as `codellama-7b.Q5_K_M.gguf`\n- **LM Studio Preset:** CodeLlama Completion\n- **FIM Template:** Select a template that matches the model, such as `codellama` for `CodeLlama-7B-GGUF` or `deepseek` for `deepseek-coder:6.7b-base-q5_K_M`.\n  \n#### Chat\n\n- **Hostname:** `localhost`\n- **Port:** `1234`\n- **Path:** `/v1/chat/completions`\n- **Model Name:** `codellama:7b-instruct` or your preferred instruct model.\n- **LM Studio Preset:** Default or `CodeLlama Instruct`\n\n### LiteLLM\n\n#### Auto-complete\n\nLiteLLM technically supports auto-complete using the `custom-template` FIM template, and by editing the `fim.hbs` file, however result will vary depending on your model and setup.\n\n#### Chat\n\n- **Hostname:** `localhost`\n- **Port:** `4000`\n- **Path:** `/v1/chat/completions`\n\nStart LiteLLM with the following command:\n\n```bash\nlitellm --model gpt-4-turbo\n```\n\n### Llama.cpp\n\n#### Auto-complete\n\nStart Llama.cpp in the terminal with this Docker command:\n\nFor example using Docker and `codellama-7b.Q5_K_M.gguf`\n\n```bash\ndocker run -p 8080:8080 --gpus all --network bridge -v /path/to/your/models:/models local/llama.cpp:full-cuda --server -m /models/codellama-7b.Q5_K_M.gguf -c 2048 -ngl 43 -mg 1 --port 8080 --host 0.0.0.0\n```\n\nConfigure your provider settings as follows:\n\n- **Hostname:** `localhost`\n- **Port:** `8080`\n- **Path:** `/completion`\n- **FIM Template:** Select a template that matches the model, such as `codellama` for `CodeLlama-7B-GGUF` or `deepseek` for `deepseek-coder:6.7b-base-q5_K_M`.\n\n#### Chat\n\nThe performance of chat functionalities with Llama.cpp has been mixed. If you obtain favorable results, please share them by opening an issue or a pull request.\n\n- **Hostname:** `localhost`\n- **Port:** `8080`\n- **Path:** `/completion`\n- **Model Name:** `CodeLlama-7B-GGUF` or any other strong instruct model.\n\n\n### Oobabooga\n\n```bash\nbash start_linux.sh --api --listen\n```\n\n#### Auto-complete\n\nNavigate to `http://0.0.0.0:7860/` and load your model:\n\n- **Hostname:** `localhost`\n- **Port:** `5000`\n- **Path:** `/v1/completions`\n- **Model Name:** `CodeLlama-7B-GGUF` or another effective instruct model.\n- **FIM Template:** Select a template that matches the model, such as `codellama` for `CodeLlama-7B-GGUF` or `deepseek` for `deepseek-coder:6.7b-base-q5_K_M`.\n\n#### Chat\n\nChat functionality has not been successful on Linux with Oobabooga:\n\n- **Hostname:** `localhost`\n- **Port:** `5000`\n- **Path:** `/v1/chat/completions`\n- **Model Name:** `CodeLlama-7B-GGUF`\n\n### Symmetry\n\nSymmetry is a distributed tool that allows you to connect to a network of computational resources. It can be used as an inference provider for twinny, offering access to a variety of models through its peer-to-peer network.\n\n#### Using Symmetry\n\n1. In the twinny extension settings, select your desired model.\n2. Click the \"Connect to Symmetry\" button in the extension.\n3. The extension will automatically connect to the Symmetry network using the selected model.\n\nThis streamlined process allows you to easily tap into the Symmetry network without manual configuration.\n\n> Note: When using Symmetry, be aware that your requests are processed by other nodes in the network. Consider the sensitivity of your data and choose trusted providers when necessary.";
						const data = {title:"Inference providers",description:"Inference providers are a way to connect twinny with external models and services.",editUrl:true,head:[],template:"doc",sidebar:{hidden:false,attrs:{}},pagefind:true};
						const _internal = {
							type: 'content',
							filePath: "/home/richard/Desktop/twinny/twinny-docs/src/content/docs/general/providers.md",
							rawData: undefined,
						};

export { _internal, body, collection, data, id, slug };
