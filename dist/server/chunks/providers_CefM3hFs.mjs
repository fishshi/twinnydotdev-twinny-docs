const id = "zh-cn/general/providers.md";
						const collection = "docs";
						const slug = "zh-cn/general/providers";
						const body = "\n这些示例配置作为起点，具体的调整可能需要根据您的硬件和软件环境进行。\n\n\n> 注意：Twinny 聊天（非自动补全）应与任何符合 OpenAI API 规范的 API 兼容。\n\n\n### Ollama（默认配置）\n\n#### 自动补全\n\n- **主机名：** `localhost`\n- **端口：** `11434`\n- **路径：** `/api/generate`\n- **模型名称：** `codellama:7b-code`\n- **FIM 模板：** `codellama`\n\n#### 聊天\n\n- **主机名：** `localhost`\n- **端口：** `11434`\n- **路径：** `/v1/chat/completions`\n- **模型名称：** `codellama:7b-instruct` \n\n### 使用 Ollama 打开 WebUI\n\nOpen WebUI 可作为 Ollama 的代理，简单地配置端点以匹配 OpenWeb UI 提供的服务。\n\n#### 自动补全\n\n- **主机名：** `localhost`\n- **端口：** OpenWebUI 服务的端口，通常为 `8080` 或 `3000`。\n- **路径：** `/ollama/api/generate`\n- **模型名称：** `codellama:7b-code`\n- **FIM 模板：** 选择一个与模型匹配的模板，如 `codellama` 用于 `codellama:7b-code` 或 `deepseek` 用于 `deepseek-coder`。\n\n#### 聊天\n\n- **主机名：** `localhost`\n- **端口：** OpenWebUI 服务的端口，通常为 `8080` 或 `3000`。\n- **路径：** `/ollama/v1/chat/completions`\n- **模型名称：** `codellama:7b-instruct` 或任何有效的指令模型。\n\n### LM Studio\n\n#### 自动补全\n\n- **主机名：** `localhost`\n- **端口：** `1234`\n- **路径：** `/v1/completions`\n- **模型名称：** 基础模型，例如 `codellama-7b.Q5_K_M.gguf`\n- **LM Studio 预设：** CodeLlama Completion\n- **FIM 模板：** 选择一个与模型匹配的模板，如 `codellama` 用于 `CodeLlama-7B-GGUF` 或 `deepseek` 用于 `deepseek-coder:6.7b-base-q5_K_M`。\n\n#### 聊天\n\n- **主机名：** `localhost`\n- **端口：** `1234`\n- **路径：** `/v1/chat/completions`\n- **模型名称：** `codellama:7b-instruct` 或您偏好的指令模型。\n- **LM Studio 预设：** 默认或 `CodeLlama Instruct`\n\n### LiteLLM\n\n#### 自动补全\n\nLiteLLM 技术上支持使用 `custom-template` FIM 模板进行自动补全，并通过编辑 `fim.hbs` 文件实现，然而结果将根据您的模型和设置有所不同。\n\n#### 聊天\n\n- **主机名：** `localhost`\n- **端口：** `4000`\n- **路径：** `/v1/chat/completions`\n\n启动 LiteLLM 使用以下命令：\n\n```bash\nlitellm --model gpt-4-turbo\n```\n\n### Llama.cpp\n\n#### 自动补全\n\n在终端中使用以下 Docker 命令启动 Llama.cpp：\n\n例如，使用 Docker 和 `codellama-7b.Q5_K_M.gguf`：\n\n```bash\ndocker run -p 8080:8080 --gpus all --network bridge -v /path/to/your/models:/models local/llama.cpp:full-cuda --server -m /models/codellama-7b.Q5_K_M.gguf -c 2048 -ngl 43 -mg 1 --port 8080 --host 0.0.0.0\n```\n\n配置您的提供商设置如下：\n\n- **主机名：** `localhost`\n- **端口：** `8080`\n- **路径：** `/completion`\n- **FIM 模板：** 选择一个与模型匹配的模板，如 `codellama` 用于 `CodeLlama-7B-GGUF` 或 `deepseek` 用于 `deepseek-coder:6.7b-base-q5_K_M`。\n\n#### 聊天\n\nLlama.cpp 的聊天功能表现不稳定。如果您获得了良好的结果，请通过打开问题或拉取请求与我们分享。\n\n- **主机名：** `localhost`\n- **端口：** `8080`\n- **路径：** `/completion`\n- **模型名称：** `CodeLlama-7B-GGUF` 或其他强大的指令模型。\n\n\n### Oobabooga\n\n```bash\nbash start_linux.sh --api --listen\n```\n\n#### 自动补全\n\n访问 `http://0.0.0.0:7860/` 并加载您的模型：\n\n- **主机名：** `localhost`\n- **端口：** `5000`\n- **路径：** `/v1/completions`\n- **模型名称：** `CodeLlama-7B-GGUF` 或其他有效的指令模型。\n- **FIM 模板：** 选择一个与模型匹配的模板，如 `codellama` 用于 `CodeLlama-7B-GGUF` 或 `deepseek` 用于 `deepseek-coder:6.7b-base-q5_K_M`。\n\n#### 聊天\n\n在 Linux 上，Oobabooga 的聊天功能未能成功：\n\n- **主机名：** `localhost`\n- **端口：** `5000`\n- **路径：** `/v1/chat/completions`\n- **模型名称：** `CodeLlama-7B-GGUF`\n\n### Symmetry\n\nSymmetry 是一款去中心化工具，允许您连接到计算资源网络。它可以作为 Twinny 的推理提供商，通过其点对点网络提供访问多种模型的功能。\n\n#### 使用 Symmetry\n\n1. 在 Twinny 扩展设置中，选择您想要的模型。\n2. 点击扩展中的 \"Connect to Symmetry\" 按钮。\n3. 扩展将自动连接到 Symmetry 网络，并使用所选模型。\n\n这个简化的过程让您可以轻松连接到 Symmetry 网络，无需手动配置。\n\n> 注意：使用 Symmetry 时，请注意您的请求是由网络中的其他节点处理的。在必要时，请考虑数据的敏感性，并选择可信的提供商。";
						const data = {title:"推理提供者",description:"推理提供者是将 Twinny 与外部模型和服务连接的一种方式。",editUrl:true,head:[],template:"doc",sidebar:{hidden:false,attrs:{}},pagefind:true};
						const _internal = {
							type: 'content',
							filePath: "/home/richard/Desktop/twinny/twinny-docs/src/content/docs/zh-cn/general/providers.md",
							rawData: undefined,
						};

export { _internal, body, collection, data, id, slug };
