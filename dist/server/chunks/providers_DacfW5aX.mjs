import { e as createComponent, r as renderTemplate, m as maybeRenderHead, u as unescapeHTML } from './astro_D8JpLML5.mjs';
import 'kleur/colors';
import 'clsx';
import 'cssesc';

const html = "<p>These example configurations serve as a starting point. Individual adjustments may be required depending on your specific hardware and software environments.</p>\n<blockquote>\n<p>Note: twinny chat (not auto-complete) should be compatible with any API which adheres to the OpenAI API specification.</p>\n</blockquote>\n<h3 id=\"ollama-configured-by-default\">Ollama (Configured by default)</h3>\n<h4 id=\"auto-complete\">Auto-complete</h4>\n<ul>\n<li><strong>Hostname:</strong> <code dir=\"auto\">localhost</code></li>\n<li><strong>Port:</strong> <code dir=\"auto\">11434</code></li>\n<li><strong>Path:</strong> <code dir=\"auto\">/api/generate</code></li>\n<li><strong>Model Name:</strong> <code dir=\"auto\">codellama:7b-code</code></li>\n<li><strong>FIM Template:</strong> <code dir=\"auto\">codellama</code></li>\n</ul>\n<h4 id=\"chat\">Chat</h4>\n<ul>\n<li><strong>Hostname:</strong> <code dir=\"auto\">localhost</code></li>\n<li><strong>Port:</strong> <code dir=\"auto\">11434</code></li>\n<li><strong>Path:</strong> <code dir=\"auto\">/v1/chat/completions</code></li>\n<li><strong>Model Name:</strong> <code dir=\"auto\">codellama:7b-instruct</code></li>\n</ul>\n<h3 id=\"open-webui-using-ollama\">Open WebUI using Ollama</h3>\n<p>Open WebUI can be used a proxy Ollama, simply configure the endpoint to match what is served by OpenWeb UI.</p>\n<h4 id=\"auto-complete-1\">Auto-complete</h4>\n<ul>\n<li><strong>Hostname:</strong> <code dir=\"auto\">localhost</code></li>\n<li><strong>Port:</strong> The port OpenWebUI is serving, typically <code dir=\"auto\">8080</code> or <code dir=\"auto\">3000</code>.</li>\n<li><strong>Path:</strong> <code dir=\"auto\">/ollama/api/generate</code></li>\n<li><strong>Model Name:</strong> <code dir=\"auto\">codellama:7b-code</code></li>\n<li><strong>FIM Template:</strong> Select a template that matches the model, such as <code dir=\"auto\">codellama</code> for <code dir=\"auto\">codellama:7b-code</code> or <code dir=\"auto\">deepseek</code> for <code dir=\"auto\">deepseek-coder</code>.</li>\n</ul>\n<h4 id=\"chat-1\">Chat</h4>\n<ul>\n<li><strong>Hostname:</strong> <code dir=\"auto\">localhost</code></li>\n<li><strong>Port:</strong> The port OpenWebUI is serving, typically <code dir=\"auto\">8080</code> or <code dir=\"auto\">3000</code>.</li>\n<li><strong>Path:</strong> <code dir=\"auto\">/ollama/v1/chat/completions</code></li>\n<li><strong>Model Name:</strong> <code dir=\"auto\">codellama:7b-instruct</code> or any effective instruct model.</li>\n</ul>\n<h3 id=\"lm-studio\">LM Studio</h3>\n<h4 id=\"auto-complete-2\">Auto-complete</h4>\n<ul>\n<li><strong>Hostname:</strong> <code dir=\"auto\">localhost</code></li>\n<li><strong>Port:</strong> <code dir=\"auto\">1234</code></li>\n<li><strong>Path:</strong> <code dir=\"auto\">/v1/completions</code></li>\n<li><strong>Model Name:</strong> Base model such as <code dir=\"auto\">codellama-7b.Q5_K_M.gguf</code></li>\n<li><strong>LM Studio Preset:</strong> CodeLlama Completion</li>\n<li><strong>FIM Template:</strong> Select a template that matches the model, such as <code dir=\"auto\">codellama</code> for <code dir=\"auto\">CodeLlama-7B-GGUF</code> or <code dir=\"auto\">deepseek</code> for <code dir=\"auto\">deepseek-coder:6.7b-base-q5_K_M</code>.</li>\n</ul>\n<h4 id=\"chat-2\">Chat</h4>\n<ul>\n<li><strong>Hostname:</strong> <code dir=\"auto\">localhost</code></li>\n<li><strong>Port:</strong> <code dir=\"auto\">1234</code></li>\n<li><strong>Path:</strong> <code dir=\"auto\">/v1/chat/completions</code></li>\n<li><strong>Model Name:</strong> <code dir=\"auto\">codellama:7b-instruct</code> or your preferred instruct model.</li>\n<li><strong>LM Studio Preset:</strong> Default or <code dir=\"auto\">CodeLlama Instruct</code></li>\n</ul>\n<h3 id=\"litellm\">LiteLLM</h3>\n<h4 id=\"auto-complete-3\">Auto-complete</h4>\n<p>LiteLLM technically supports auto-complete using the <code dir=\"auto\">custom-template</code> FIM template, and by editing the <code dir=\"auto\">fim.hbs</code> file, however result will vary depending on your model and setup.</p>\n<h4 id=\"chat-3\">Chat</h4>\n<ul>\n<li><strong>Hostname:</strong> <code dir=\"auto\">localhost</code></li>\n<li><strong>Port:</strong> <code dir=\"auto\">4000</code></li>\n<li><strong>Path:</strong> <code dir=\"auto\">/v1/chat/completions</code></li>\n</ul>\n<p>Start LiteLLM with the following command:</p>\n<div class=\"expressive-code\"><link rel=\"stylesheet\" href=\"/_astro/ec.d6kn2.css\"><script type=\"module\" src=\"/_astro/ec.dy9ns.js\"></script><figure class=\"frame is-terminal not-content\"><figcaption class=\"header\"><span class=\"title\"></span><span class=\"sr-only\">Terminal window</span></figcaption><pre tabindex=\"0\"><code><div class=\"ec-line\"><div class=\"code\"><span style=\"--0:#82AAFF;--1:#3C63B3\">litellm</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#82AAFF;--1:#3C63B3\">--model</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#ECC48D;--1:#3C63B3\">gpt-4-turbo</span></div></div></code></pre><div class=\"copy\"><button title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"litellm --model gpt-4-turbo\"><div></div></button></div></figure></div>\n<h3 id=\"llamacpp\">Llama.cpp</h3>\n<h4 id=\"auto-complete-4\">Auto-complete</h4>\n<p>Start Llama.cpp in the terminal with this Docker command:</p>\n<p>For example using Docker and <code dir=\"auto\">codellama-7b.Q5_K_M.gguf</code></p>\n<div class=\"expressive-code\"><figure class=\"frame is-terminal not-content\"><figcaption class=\"header\"><span class=\"title\"></span><span class=\"sr-only\">Terminal window</span></figcaption><pre tabindex=\"0\"><code><div class=\"ec-line\"><div class=\"code\"><span style=\"--0:#82AAFF;--1:#3C63B3\">docker</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#ECC48D;--1:#3C63B3\">run</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#82AAFF;--1:#3C63B3\">-p</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#ECC48D;--1:#3C63B3\">8080:8080</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#82AAFF;--1:#3C63B3\">--gpus</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#ECC48D;--1:#3C63B3\">all</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#82AAFF;--1:#3C63B3\">--network</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#ECC48D;--1:#3C63B3\">bridge</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#82AAFF;--1:#3C63B3\">-v</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#ECC48D;--1:#3C63B3\">/path/to/your/models:/models</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#ECC48D;--1:#3C63B3\">local/llama.cpp:full-cuda</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#82AAFF;--1:#3C63B3\">--server</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#82AAFF;--1:#3C63B3\">-m</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#ECC48D;--1:#3C63B3\">/models/codellama-7b.Q5_K_M.gguf</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#82AAFF;--1:#3C63B3\">-c</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#F78C6C;--1:#AA0982\">2048</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#82AAFF;--1:#3C63B3\">-ngl</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#F78C6C;--1:#AA0982\">43</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#82AAFF;--1:#3C63B3\">-mg</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#F78C6C;--1:#AA0982\">1</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#82AAFF;--1:#3C63B3\">--port</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#F78C6C;--1:#AA0982\">8080</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#82AAFF;--1:#3C63B3\">--host</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#F78C6C;--1:#AA0982\">0.0.0.0</span></div></div></code></pre><div class=\"copy\"><button title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"docker run -p 8080:8080 --gpus all --network bridge -v /path/to/your/models:/models local/llama.cpp:full-cuda --server -m /models/codellama-7b.Q5_K_M.gguf -c 2048 -ngl 43 -mg 1 --port 8080 --host 0.0.0.0\"><div></div></button></div></figure></div>\n<p>Configure your provider settings as follows:</p>\n<ul>\n<li><strong>Hostname:</strong> <code dir=\"auto\">localhost</code></li>\n<li><strong>Port:</strong> <code dir=\"auto\">8080</code></li>\n<li><strong>Path:</strong> <code dir=\"auto\">/completion</code></li>\n<li><strong>FIM Template:</strong> Select a template that matches the model, such as <code dir=\"auto\">codellama</code> for <code dir=\"auto\">CodeLlama-7B-GGUF</code> or <code dir=\"auto\">deepseek</code> for <code dir=\"auto\">deepseek-coder:6.7b-base-q5_K_M</code>.</li>\n</ul>\n<h4 id=\"chat-4\">Chat</h4>\n<p>The performance of chat functionalities with Llama.cpp has been mixed. If you obtain favorable results, please share them by opening an issue or a pull request.</p>\n<ul>\n<li><strong>Hostname:</strong> <code dir=\"auto\">localhost</code></li>\n<li><strong>Port:</strong> <code dir=\"auto\">8080</code></li>\n<li><strong>Path:</strong> <code dir=\"auto\">/completion</code></li>\n<li><strong>Model Name:</strong> <code dir=\"auto\">CodeLlama-7B-GGUF</code> or any other strong instruct model.</li>\n</ul>\n<h3 id=\"oobabooga\">Oobabooga</h3>\n<div class=\"expressive-code\"><figure class=\"frame is-terminal not-content\"><figcaption class=\"header\"><span class=\"title\"></span><span class=\"sr-only\">Terminal window</span></figcaption><pre tabindex=\"0\"><code><div class=\"ec-line\"><div class=\"code\"><span style=\"--0:#82AAFF;--1:#3C63B3\">bash</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#ECC48D;--1:#3C63B3\">start_linux.sh</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#82AAFF;--1:#3C63B3\">--api</span><span style=\"--0:#D6DEEB;--1:#403F53\"> </span><span style=\"--0:#82AAFF;--1:#3C63B3\">--listen</span></div></div></code></pre><div class=\"copy\"><button title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"bash start_linux.sh --api --listen\"><div></div></button></div></figure></div>\n<h4 id=\"auto-complete-5\">Auto-complete</h4>\n<p>Navigate to <code dir=\"auto\">http://0.0.0.0:7860/</code> and load your model:</p>\n<ul>\n<li><strong>Hostname:</strong> <code dir=\"auto\">localhost</code></li>\n<li><strong>Port:</strong> <code dir=\"auto\">5000</code></li>\n<li><strong>Path:</strong> <code dir=\"auto\">/v1/completions</code></li>\n<li><strong>Model Name:</strong> <code dir=\"auto\">CodeLlama-7B-GGUF</code> or another effective instruct model.</li>\n<li><strong>FIM Template:</strong> Select a template that matches the model, such as <code dir=\"auto\">codellama</code> for <code dir=\"auto\">CodeLlama-7B-GGUF</code> or <code dir=\"auto\">deepseek</code> for <code dir=\"auto\">deepseek-coder:6.7b-base-q5_K_M</code>.</li>\n</ul>\n<h4 id=\"chat-5\">Chat</h4>\n<p>Chat functionality has not been successful on Linux with Oobabooga:</p>\n<ul>\n<li><strong>Hostname:</strong> <code dir=\"auto\">localhost</code></li>\n<li><strong>Port:</strong> <code dir=\"auto\">5000</code></li>\n<li><strong>Path:</strong> <code dir=\"auto\">/v1/chat/completions</code></li>\n<li><strong>Model Name:</strong> <code dir=\"auto\">CodeLlama-7B-GGUF</code></li>\n</ul>\n<h3 id=\"symmetry\">Symmetry</h3>\n<p>Symmetry is a distributed tool that allows you to connect to a network of computational resources. It can be used as an inference provider for twinny, offering access to a variety of models through its peer-to-peer network.</p>\n<h4 id=\"using-symmetry\">Using Symmetry</h4>\n<ol>\n<li>In the twinny extension settings, select your desired model.</li>\n<li>Click the “Connect to Symmetry” button in the extension.</li>\n<li>The extension will automatically connect to the Symmetry network using the selected model.</li>\n</ol>\n<p>This streamlined process allows you to easily tap into the Symmetry network without manual configuration.</p>\n<blockquote>\n<p>Note: When using Symmetry, be aware that your requests are processed by other nodes in the network. Consider the sensitivity of your data and choose trusted providers when necessary.</p>\n</blockquote>";

				const frontmatter = {"title":"Inference providers","description":"Inference providers are a way to connect twinny with external models and services."};
				const file = "/home/richard/Desktop/twinny/twinny-docs/src/content/docs/general/providers.md";
				const url = undefined;
				function rawContent() {
					return "\nThese example configurations serve as a starting point. Individual adjustments may be required depending on your specific hardware and software environments.\n\n\n> Note: twinny chat (not auto-complete) should be compatible with any API which adheres to the OpenAI API specification.\n\n\n### Ollama (Configured by default)\n\n#### Auto-complete\n\n- **Hostname:** `localhost`\n- **Port:** `11434`\n- **Path:** `/api/generate`\n- **Model Name:** `codellama:7b-code`\n- **FIM Template:** `codellama`\n\n#### Chat\n\n- **Hostname:** `localhost`\n- **Port:** `11434`\n- **Path:** `/v1/chat/completions`\n- **Model Name:** `codellama:7b-instruct` \n\n### Open WebUI using Ollama\n\nOpen WebUI can be used a proxy Ollama, simply configure the endpoint to match what is served by OpenWeb UI.\n\n#### Auto-complete\n\n- **Hostname:** `localhost`\n- **Port:** The port OpenWebUI is serving, typically `8080` or `3000`.\n- **Path:** `/ollama/api/generate`\n- **Model Name:** `codellama:7b-code`\n- **FIM Template:** Select a template that matches the model, such as `codellama` for `codellama:7b-code` or `deepseek` for `deepseek-coder`.\n\n#### Chat\n\n- **Hostname:** `localhost`\n- **Port:** The port OpenWebUI is serving, typically `8080` or `3000`.\n- **Path:** `/ollama/v1/chat/completions`\n- **Model Name:** `codellama:7b-instruct` or any effective instruct model.\n\n### LM Studio\n\n#### Auto-complete\n\n- **Hostname:** `localhost`\n- **Port:** `1234`\n- **Path:** `/v1/completions`\n- **Model Name:** Base model such as `codellama-7b.Q5_K_M.gguf`\n- **LM Studio Preset:** CodeLlama Completion\n- **FIM Template:** Select a template that matches the model, such as `codellama` for `CodeLlama-7B-GGUF` or `deepseek` for `deepseek-coder:6.7b-base-q5_K_M`.\n  \n#### Chat\n\n- **Hostname:** `localhost`\n- **Port:** `1234`\n- **Path:** `/v1/chat/completions`\n- **Model Name:** `codellama:7b-instruct` or your preferred instruct model.\n- **LM Studio Preset:** Default or `CodeLlama Instruct`\n\n### LiteLLM\n\n#### Auto-complete\n\nLiteLLM technically supports auto-complete using the `custom-template` FIM template, and by editing the `fim.hbs` file, however result will vary depending on your model and setup.\n\n#### Chat\n\n- **Hostname:** `localhost`\n- **Port:** `4000`\n- **Path:** `/v1/chat/completions`\n\nStart LiteLLM with the following command:\n\n```bash\nlitellm --model gpt-4-turbo\n```\n\n### Llama.cpp\n\n#### Auto-complete\n\nStart Llama.cpp in the terminal with this Docker command:\n\nFor example using Docker and `codellama-7b.Q5_K_M.gguf`\n\n```bash\ndocker run -p 8080:8080 --gpus all --network bridge -v /path/to/your/models:/models local/llama.cpp:full-cuda --server -m /models/codellama-7b.Q5_K_M.gguf -c 2048 -ngl 43 -mg 1 --port 8080 --host 0.0.0.0\n```\n\nConfigure your provider settings as follows:\n\n- **Hostname:** `localhost`\n- **Port:** `8080`\n- **Path:** `/completion`\n- **FIM Template:** Select a template that matches the model, such as `codellama` for `CodeLlama-7B-GGUF` or `deepseek` for `deepseek-coder:6.7b-base-q5_K_M`.\n\n#### Chat\n\nThe performance of chat functionalities with Llama.cpp has been mixed. If you obtain favorable results, please share them by opening an issue or a pull request.\n\n- **Hostname:** `localhost`\n- **Port:** `8080`\n- **Path:** `/completion`\n- **Model Name:** `CodeLlama-7B-GGUF` or any other strong instruct model.\n\n\n### Oobabooga\n\n```bash\nbash start_linux.sh --api --listen\n```\n\n#### Auto-complete\n\nNavigate to `http://0.0.0.0:7860/` and load your model:\n\n- **Hostname:** `localhost`\n- **Port:** `5000`\n- **Path:** `/v1/completions`\n- **Model Name:** `CodeLlama-7B-GGUF` or another effective instruct model.\n- **FIM Template:** Select a template that matches the model, such as `codellama` for `CodeLlama-7B-GGUF` or `deepseek` for `deepseek-coder:6.7b-base-q5_K_M`.\n\n#### Chat\n\nChat functionality has not been successful on Linux with Oobabooga:\n\n- **Hostname:** `localhost`\n- **Port:** `5000`\n- **Path:** `/v1/chat/completions`\n- **Model Name:** `CodeLlama-7B-GGUF`\n\n### Symmetry\n\nSymmetry is a distributed tool that allows you to connect to a network of computational resources. It can be used as an inference provider for twinny, offering access to a variety of models through its peer-to-peer network.\n\n#### Using Symmetry\n\n1. In the twinny extension settings, select your desired model.\n2. Click the \"Connect to Symmetry\" button in the extension.\n3. The extension will automatically connect to the Symmetry network using the selected model.\n\nThis streamlined process allows you to easily tap into the Symmetry network without manual configuration.\n\n> Note: When using Symmetry, be aware that your requests are processed by other nodes in the network. Consider the sensitivity of your data and choose trusted providers when necessary.";
				}
				function compiledContent() {
					return html;
				}
				function getHeadings() {
					return [{"depth":3,"slug":"ollama-configured-by-default","text":"Ollama (Configured by default)"},{"depth":4,"slug":"auto-complete","text":"Auto-complete"},{"depth":4,"slug":"chat","text":"Chat"},{"depth":3,"slug":"open-webui-using-ollama","text":"Open WebUI using Ollama"},{"depth":4,"slug":"auto-complete-1","text":"Auto-complete"},{"depth":4,"slug":"chat-1","text":"Chat"},{"depth":3,"slug":"lm-studio","text":"LM Studio"},{"depth":4,"slug":"auto-complete-2","text":"Auto-complete"},{"depth":4,"slug":"chat-2","text":"Chat"},{"depth":3,"slug":"litellm","text":"LiteLLM"},{"depth":4,"slug":"auto-complete-3","text":"Auto-complete"},{"depth":4,"slug":"chat-3","text":"Chat"},{"depth":3,"slug":"llamacpp","text":"Llama.cpp"},{"depth":4,"slug":"auto-complete-4","text":"Auto-complete"},{"depth":4,"slug":"chat-4","text":"Chat"},{"depth":3,"slug":"oobabooga","text":"Oobabooga"},{"depth":4,"slug":"auto-complete-5","text":"Auto-complete"},{"depth":4,"slug":"chat-5","text":"Chat"},{"depth":3,"slug":"symmetry","text":"Symmetry"},{"depth":4,"slug":"using-symmetry","text":"Using Symmetry"}];
				}

				const Content = createComponent((result, _props, slots) => {
					const { layout, ...content } = frontmatter;
					content.file = file;
					content.url = url;

					return renderTemplate`${maybeRenderHead()}${unescapeHTML(html)}`;
				});

export { Content, compiledContent, Content as default, file, frontmatter, getHeadings, rawContent, url };
