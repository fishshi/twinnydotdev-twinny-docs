import { e as createComponent, r as renderTemplate, m as maybeRenderHead, u as unescapeHTML } from './astro_D8JpLML5.mjs';
import 'kleur/colors';
import 'clsx';
import 'cssesc';

const html = "<p>twinny is a configurable extension/interface which means many models are technically supported. However, not all models work well with twinny in certain scenarios.  The following is a list of the models that have been tested and found to work well with twinny.  If you find a model that works but is not listed here, please let us know so we can add it to the list or open a pull request to add it.</p>\n<h3 id=\"chat\">Chat</h3>\n<p>In theory any chat model which is trained for instructing will work with twinny.  The following are some example of models recommended for chat.</p>\n<ul>\n<li><a href=\"https://ollama.com/library/llama3.1\"><code dir=\"auto\">llama3.1</code></a></li>\n<li><a href=\"https://ollama.com/library/codellama:instruct\"><code dir=\"auto\">codellama:7b-instruct</code></a></li>\n<li><a href=\"https://ollama.com/library/phind-codellama\"><code dir=\"auto\">phind-codellama</code></a></li>\n<li><a href=\"https://ollama.com/library/mistral\"><code dir=\"auto\">mistral</code></a></li>\n<li><a href=\"https://ollama.com/library/qwen2.5-coder:7b-instruct\"><code dir=\"auto\">qwen2.5-coder</code></a></li>\n<li><a href=\"https://ollama.com/library/codestral\"><code dir=\"auto\">codestral</code></a></li>\n</ul>\n<h3 id=\"fill-in-middle\">Fill-in-middle</h3>\n<p>Only certain models support fill in the middle due to their training data.  The following are some example of models recommended for fill in the middle.  If you find a model that works but is not listed here, please let us know so we can add it to the list or open a pull request to add it.</p>\n<h4 id=\"qwen25-coder-models\">Qwen2.5-coder models</h4>\n<ul>\n<li><a href=\"https://ollama.com/library/qwen2.5-coder:7b-base\"><code dir=\"auto\">qwen2.5-coder:7b-base</code></a></li>\n</ul>\n<h4 id=\"codellama-models\">Codellama models</h4>\n<p><code dir=\"auto\">code</code> versions of codellama models.</p>\n<ul>\n<li><a href=\"https://ollama.com/library/codellama:code\"><code dir=\"auto\">codellama:code</code></a></li>\n<li><a href=\"https://ollama.com/library/codellama:13b-code\"><code dir=\"auto\">codellama:13b-code</code></a></li>\n</ul>\n<p>Note: The <em>34b</em> version of codellama does not work well with fill in the middle.</p>\n<h4 id=\"deepseek-coder-models\">Deepseek Coder models</h4>\n<p><code dir=\"auto\">base</code> versions of deepseek-coder models.</p>\n<ul>\n<li><a href=\"https://ollama.com/library/deepseek-coder:base\"><code dir=\"auto\">deepseek-coder:base</code></a></li>\n</ul>\n<p>Note: Models which are not base versions do not work well with fill in the middle.</p>\n<h4 id=\"starcoder-models\">Starcoder models</h4>\n<p><code dir=\"auto\">base</code> versions of starcoder models. The default and base models are the same.</p>\n<ul>\n<li><a href=\"https://ollama.com/library/starcoder\"><code dir=\"auto\">starcoder</code></a></li>\n<li><a href=\"https://ollama.com/library/starcoder2\"><code dir=\"auto\">starcoder2</code></a></li>\n</ul>\n<p>Note: Starcoder2 doesn’t always stop when it is finished.  Lowering the temperature and upping the repeat penalty helps with this issue.</p>\n<p>Use <a href=\"https://ollama.com/library/starcoder2:7b\">Starcoder2 7b</a> for best results.</p>\n<h4 id=\"stablecode-models\">Stablecode models</h4>\n<p><code dir=\"auto\">code</code> versions of stablecode models.</p>\n<ul>\n<li><a href=\"https://ollama.com/library/stable-code:3b-code\"><code dir=\"auto\">stable-code:3b-code</code></a></li>\n</ul>\n<h4 id=\"codegemma-models\">Codegemma models</h4>\n<p><code dir=\"auto\">code</code> versions of codegemma models.</p>\n<ul>\n<li><a href=\"https://ollama.com/library/codegemma:7b-code\"><code dir=\"auto\">codegemma</code></a></li>\n</ul>\n<p>Note: CodeGemma doesn’t always stop when it is finished.  Lowering the temperature and upping the repeat penalty helps with this issue.</p>";

				const frontmatter = {"title":"Supported models","description":"A list of supported models for twinny."};
				const file = "/home/richard/Desktop/twinny/twinny-docs/src/content/docs/general/supported-models.md";
				const url = undefined;
				function rawContent() {
					return "\ntwinny is a configurable extension/interface which means many models are technically supported. However, not all models work well with twinny in certain scenarios.  The following is a list of the models that have been tested and found to work well with twinny.  If you find a model that works but is not listed here, please let us know so we can add it to the list or open a pull request to add it.\n\n### Chat\n\nIn theory any chat model which is trained for instructing will work with twinny.  The following are some example of models recommended for chat.\n\n\n- [`llama3.1`](https://ollama.com/library/llama3.1)\n- [`codellama:7b-instruct`](https://ollama.com/library/codellama:instruct)\n- [`phind-codellama`](https://ollama.com/library/phind-codellama)\n- [`mistral`](https://ollama.com/library/mistral)\n- [`qwen2.5-coder`](https://ollama.com/library/qwen2.5-coder:7b-instruct)\n- [`codestral`](https://ollama.com/library/codestral)\n\n### Fill-in-middle\n\nOnly certain models support fill in the middle due to their training data.  The following are some example of models recommended for fill in the middle.  If you find a model that works but is not listed here, please let us know so we can add it to the list or open a pull request to add it.\n\n#### Qwen2.5-coder models\n\n- [`qwen2.5-coder:7b-base`](https://ollama.com/library/qwen2.5-coder:7b-base)\n\n#### Codellama models\n\n`code` versions of codellama models.\n\n- [`codellama:code`](https://ollama.com/library/codellama:code)\n- [`codellama:13b-code`](https://ollama.com/library/codellama:13b-code)\n  \nNote: The _34b_ version of codellama does not work well with fill in the middle.\n\n#### Deepseek Coder models\n\n`base` versions of deepseek-coder models.\n\n- [`deepseek-coder:base`](https://ollama.com/library/deepseek-coder:base)\n\nNote: Models which are not base versions do not work well with fill in the middle.\n\n#### Starcoder models\n\n`base` versions of starcoder models. The default and base models are the same.\n\n- [`starcoder`](https://ollama.com/library/starcoder)\n- [`starcoder2`](https://ollama.com/library/starcoder2)\n\nNote: Starcoder2 doesn't always stop when it is finished.  Lowering the temperature and upping the repeat penalty helps with this issue.\n\nUse [Starcoder2 7b](https://ollama.com/library/starcoder2:7b) for best results.\n\n#### Stablecode models\n\n`code` versions of stablecode models.\n\n- [`stable-code:3b-code`](https://ollama.com/library/stable-code:3b-code)\n\n#### Codegemma models\n\n`code` versions of codegemma models.\n\n- [`codegemma`](https://ollama.com/library/codegemma:7b-code)\n\nNote: CodeGemma doesn't always stop when it is finished.  Lowering the temperature and upping the repeat penalty helps with this issue.";
				}
				function compiledContent() {
					return html;
				}
				function getHeadings() {
					return [{"depth":3,"slug":"chat","text":"Chat"},{"depth":3,"slug":"fill-in-middle","text":"Fill-in-middle"},{"depth":4,"slug":"qwen25-coder-models","text":"Qwen2.5-coder models"},{"depth":4,"slug":"codellama-models","text":"Codellama models"},{"depth":4,"slug":"deepseek-coder-models","text":"Deepseek Coder models"},{"depth":4,"slug":"starcoder-models","text":"Starcoder models"},{"depth":4,"slug":"stablecode-models","text":"Stablecode models"},{"depth":4,"slug":"codegemma-models","text":"Codegemma models"}];
				}

				const Content = createComponent((result, _props, slots) => {
					const { layout, ...content } = frontmatter;
					content.file = file;
					content.url = url;

					return renderTemplate`${maybeRenderHead()}${unescapeHTML(html)}`;
				});

export { Content, compiledContent, Content as default, file, frontmatter, getHeadings, rawContent, url };
