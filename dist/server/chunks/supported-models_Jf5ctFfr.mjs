const id = "general/supported-models.md";
						const collection = "docs";
						const slug = "general/supported-models";
						const body = "\ntwinny is a configurable extension/interface which means many models are technically supported. However, not all models work well with twinny in certain scenarios.  The following is a list of the models that have been tested and found to work well with twinny.  If you find a model that works but is not listed here, please let us know so we can add it to the list or open a pull request to add it.\n\n### Chat\n\nIn theory any chat model which is trained for instructing will work with twinny.  The following are some example of models recommended for chat.\n\n\n- [`llama3.1`](https://ollama.com/library/llama3.1)\n- [`codellama:7b-instruct`](https://ollama.com/library/codellama:instruct)\n- [`phind-codellama`](https://ollama.com/library/phind-codellama)\n- [`mistral`](https://ollama.com/library/mistral)\n- [`qwen2.5-coder`](https://ollama.com/library/qwen2.5-coder:7b-instruct)\n- [`codestral`](https://ollama.com/library/codestral)\n\n### Fill-in-middle\n\nOnly certain models support fill in the middle due to their training data.  The following are some example of models recommended for fill in the middle.  If you find a model that works but is not listed here, please let us know so we can add it to the list or open a pull request to add it.\n\n#### Qwen2.5-coder models\n\n- [`qwen2.5-coder:7b-base`](https://ollama.com/library/qwen2.5-coder:7b-base)\n\n#### Codellama models\n\n`code` versions of codellama models.\n\n- [`codellama:code`](https://ollama.com/library/codellama:code)\n- [`codellama:13b-code`](https://ollama.com/library/codellama:13b-code)\n  \nNote: The _34b_ version of codellama does not work well with fill in the middle.\n\n#### Deepseek Coder models\n\n`base` versions of deepseek-coder models.\n\n- [`deepseek-coder:base`](https://ollama.com/library/deepseek-coder:base)\n\nNote: Models which are not base versions do not work well with fill in the middle.\n\n#### Starcoder models\n\n`base` versions of starcoder models. The default and base models are the same.\n\n- [`starcoder`](https://ollama.com/library/starcoder)\n- [`starcoder2`](https://ollama.com/library/starcoder2)\n\nNote: Starcoder2 doesn't always stop when it is finished.  Lowering the temperature and upping the repeat penalty helps with this issue.\n\nUse [Starcoder2 7b](https://ollama.com/library/starcoder2:7b) for best results.\n\n#### Stablecode models\n\n`code` versions of stablecode models.\n\n- [`stable-code:3b-code`](https://ollama.com/library/stable-code:3b-code)\n\n#### Codegemma models\n\n`code` versions of codegemma models.\n\n- [`codegemma`](https://ollama.com/library/codegemma:7b-code)\n\nNote: CodeGemma doesn't always stop when it is finished.  Lowering the temperature and upping the repeat penalty helps with this issue.";
						const data = {title:"Supported models",description:"A list of supported models for twinny.",editUrl:true,head:[],template:"doc",sidebar:{hidden:false,attrs:{}},pagefind:true};
						const _internal = {
							type: 'content',
							filePath: "/home/richard/Desktop/twinny/twinny-docs/src/content/docs/general/supported-models.md",
							rawData: undefined,
						};

export { _internal, body, collection, data, id, slug };
